#apply Min-Max normalization
crxNormal[,colNumericas] <- as.data.frame(lapply(crxNormal[,colNumericas], min_max_norm))
df = iris
set.seed(1234)
#OPCION 1
ind = sample(2, nrow(df), replace = TRUE, prob = c(0.7, 0.3))
random.train = df[ind==1,]
random.test = df[ind==2,]
#OPCION 2
trainTestSplit = floor(nrow(df)*0.7)
install.packages("caret")
#OPCION 1
ind = sample(2, nrow(crxNormal), replace = TRUE, prob = c(0.7, 0.3))
random.train = crxNormal[ind==1,]
crx <- read.csv("Machine Learning/crx.data", header=FALSE)
colNumericas<-c(2, 3, 8, 11, 14, 15)
#HACER este proceso para todas las columnas que son numericas y existen missing values
#  -------------------------------------#
for (ncol in colNumericas){
# Imputar datos cuando los valores son ? #
# 1. Localizar las posiciones de las filas que tienen los missing values
posNO<-which(crx[,ncol] == "?")
# 2. Debido a que la columna es tipo factor, convertir a numerico aquellas
#    filas que son diferentes a NAÂ´s o ?Â´s
vecNumeric<-as.numeric(as.character(crx[which(crx[,ncol] != "?"),ncol]))
# 3. Obtener la media solo de los datos que no son missing values
mediaV<-mean(vecNumeric)
# 4. Convertir a numÃ©rico toda la columna deseada
crx[,ncol]<-as.numeric(as.character(crx[,ncol]))
# 5. Asignar la media de la columna deseada a las filas donde existen missing values
crx[posNO,ncol]<-mediaV
}
#funciÃ³n para normalizar
min_max_norm <- function(x) {
(x - min(x)) / (max(x) - min(x))
}
crxNormal<-crx
#apply Min-Max normalization
crxNormal[,colNumericas] <- as.data.frame(lapply(crxNormal[,colNumericas], min_max_norm))
standarizacion<-function(x) {
(x - mean(x))/sd(x)
}
crxEstandar<-crx
#estandardiza todas las columnas numÃ©ricas
crxEstandar[,colNumericas] <- as.data.frame(lapply(crxEstandar[,colNumericas], standarizacion))
summary(crx)
summary(crxNormal)
summary(crxEstandar)
#OPCION 1
ind = sample(2, nrow(crxNormal), replace = TRUE, prob = c(0.7, 0.3))
random.train = crxNormal[ind==1,]
random.test = crxNormal[ind==2,]
#OPCION 2
trainTestSplit = floor(nrow(crxNormal)*0.7)
#Asignación de los datos de entrenamiento
train = crxNormal[1:trainTestSplit,]
#Datos de prueba
test = crxNormal[-(1:trainTestSplit),]
View(random.test)
View(random.test)
View(random.test)
View(random.train)
View(test)
View(train)
crx <- read.csv("Machine Learning/crx.data", header=FALSE, na.strings="?")
colNumericas<-c(2, 3, 8, 11, 14, 15)
for (i in colNumericas){
posV<-which(is.na(crx[,i]))
mediaV<-mean(!is.na(crx[,i]))
crx[posV,i]<-mediaV
}
summary(crx)
#funciÃ³n para normalizar
min_max_norm <- function(x) {
(x - min(x)) / (max(x) - min(x))
}
crxNormal<-crx
#apply Min-Max normalization
crxNormal[,colNumericas] <- as.data.frame(lapply(crxNormal[,colNumericas], min_max_norm))
standarizacion<-function(x) {
(x - mean(x))/sd(x)
}
crxEstandar<-crx
#estandardiza todas las columnas numÃ©ricas
crxEstandar[,colNumericas] <- as.data.frame(lapply(crxEstandar[,colNumericas], standarizacion))
summary(crx)
summary(crxNormal)
summary(crxEstandar)
#OPCION 1
ind = sample(2, nrow(crxNormal), replace = TRUE, prob = c(0.7, 0.3))
random.train = crxNormal[ind==1,]
random.test = crxNormal[ind==2,]
#OPCION 2
trainTestSplit = floor(nrow(crxNormal)*0.7)
#Asignación de los datos de entrenamiento
train = crxNormal[1:trainTestSplit,]
#Datos de prueba
test = crxNormal[-(1:trainTestSplit),]
View(random.test)
View(random.train)
crx <- read.csv("Machine Learning/crx.data", header=FALSE)
colNumericas<-c(2, 3, 8, 11, 14, 15)
#HACER este proceso para todas las columnas que son numericas y existen missing values
#  -------------------------------------#
for (ncol in colNumericas){
# Imputar datos cuando los valores son ? #
# 1. Localizar las posiciones de las filas que tienen los missing values
posNO<-which(crx[,ncol] == "?")
# 2. Debido a que la columna es tipo factor, convertir a numerico aquellas
#    filas que son diferentes a NAÂ´s o ?Â´s
vecNumeric<-as.numeric(as.character(crx[which(crx[,ncol] != "?"),ncol]))
# 3. Obtener la media solo de los datos que no son missing values
mediaV<-mean(vecNumeric)
# 4. Convertir a numÃ©rico toda la columna deseada
crx[,ncol]<-as.numeric(as.character(crx[,ncol]))
# 5. Asignar la media de la columna deseada a las filas donde existen missing values
crx[posNO,ncol]<-mediaV
}
#funciÃ³n para normalizar
min_max_norm <- function(x) {
(x - min(x)) / (max(x) - min(x))
}
crxNormal<-crx
#apply Min-Max normalization
crxNormal[,colNumericas] <- as.data.frame(lapply(crxNormal[,colNumericas], min_max_norm))
standarizacion<-function(x) {
(x - mean(x))/sd(x)
}
crxEstandar<-crx
#estandardiza todas las columnas numÃ©ricas
crxEstandar[,colNumericas] <- as.data.frame(lapply(crxEstandar[,colNumericas], standarizacion))
#OPCION 1
ind = sample(2, nrow(crxNormal), replace = TRUE, prob = c(0.7, 0.3))
random.train = crxNormal[ind==1,]
random.test = crxNormal[ind==2,]
View(random.test)
View(random.train)
crx <- read.csv("Machine Learning/crx.data", header=FALSE, na.strings="?")
crx <- read.csv("Machine Learning/crx.data", header=FALSE, na.strings="?")
colNumericas<-c(2, 3, 8, 11, 14, 15)
for (i in colNumericas){
posV<-which(is.na(crx[,i]))
mediaV<-mean(!is.na(crx[,i]))
crx[posV,i]<-mediaV
}
summary(crx)
#funciÃ³n para normalizar
min_max_norm <- function(x) {
(x - min(x)) / (max(x) - min(x))
}
crxNormal<-crx
#apply Min-Max normalization
crxNormal[,colNumericas] <- as.data.frame(lapply(crxNormal[,colNumericas], min_max_norm))
standarizacion<-function(x) {
(x - mean(x))/sd(x)
}
crxEstandar<-crx
#estandardiza todas las columnas numÃ©ricas
crxEstandar[,colNumericas] <- as.data.frame(lapply(crxEstandar[,colNumericas], standarizacion))
summary(crx)
#OPCION 1
ind = sample(2, nrow(crxNormal), replace = TRUE, prob = c(0.7, 0.3))
random.train = crxNormal[ind==1,]
random.test = crxNormal[ind==2,]
View(random.test)
View(random.train)
View(random.train)
#create data frame
df <- data.frame(y=c(6, 8, 12, 14, 14, 15, 17, 22, 24, 23),
x1=c(2, 5, 4, 3, 4, 6, 7, 5, 8, 9),
x2=c(14, 12, 12, 13, 7, 8, 7, 4, 6, 5))
#view data frame
df
#create data frame
df <- data.frame(y=c(6, 8, 12, 14, 14, 15, 17, 22, 24, 23),
x1=c(2, 5, 4, 3, 4, 6, 7, 5, 8, 9),
x2=c(14, 12, 12, 13, 7, 8, 7, 4, 6, 5))
#view data frame
df
library(caret)
#specify the cross-validation method
ctrl <- trainControl(method = "cv", number = 5)
#fit a regression model and use k-fold CV to evaluate performance
model <- train(y ~ x1 + x2, data = df, method = "lm", trControl = ctrl)
#view summary of k-fold CV
print(model)
dfTemp = iris
nFolds = 5
sizeTest = nrow(dfTemp)/nFolds
sizeTrain = nrow(dfTemp)-sizeTest
indicesDFTrain = matrix(nrow = sizeTrain, ncol = nFolds)
indicesDFTest = matrix(nrow = sizeTest, ncol = nFolds)
i = 1
View(dfTemp)
View(dfTemp)
View(indicesDFTrain)
dfTemp = iris
nFolds = 5
sizeTest = nrow(dfTemp)/nFolds
sizeTrain = nrow(dfTemp)-sizeTest
indicesDFTrain = matrix(nrow = sizeTrain, ncol = nFolds)
indicesDFTest = matrix(nrow = sizeTest, ncol = nFolds)
indDF = seq(1,nrow(dfTemp))
for (i in 1:nFolds){
inicioTest = ((nFolds-i)*sizeTest)+1
indicesDFTest[,i] = seq(inicioTest, (inicioTest-1 + sizeTest))
indicesDFT = indDF[-indicesDFTest[,i]]
indicesDFTrain[,i] = indicesDFT
}
View(indicesDFTrain)
View(indicesDFTest)
#metodo KNN aplicada al conjunto iris
df = iris
set.seed(1234)
#OPCION 1
ind = sample(2, nrow(df), replace = TRUE, prob = c(0.7, 0.3))
random.train = df[ind==1,]
random.test = df[ind==2,]
#metodo KNN aplicada al conjunto iris
df = iris
set.seed(1234)
#OPCION 1
ind = sample(2, nrow(df), replace = TRUE, prob = c(0.7, 0.3))
random.train = df[ind==1,]
random.test = df[ind==2,]
instTrain = random.train[1, 1:4]
instTest = random.test[1, 1:4]
function distanciaManhattan(x, y){
result = sum(abs(x-y))
}
df = iris
set.seed(1234)
#OPCION 1
ind = sample(2, nrow(df), replace = TRUE, prob = c(0.7, 0.3))
random.train = df[ind==1,]
random.test = df[ind==2,]
instTrain = random.train[1, 1:4]
instTest = random.test[1, 1:4]
distanciaManhattan = function(x, y){
result = sum(abs(x-y))
}
dist = distanciaManhattan(instTrain, instTest)
instTrain
instTest
dist
View(df)
View(df)
View(df)
install.packages("class")
library(class)
random = runif(150)
random
data(iris)
str(iris)
summary(iris)
set.seed(1234)
head(iris)
random = runif(150)
iris_random = iris[order(random),]
iris_random
data(iris)
str(iris)
summary(iris)
set.seed(1234)
head(iris)
random = runif(150)
iris_random = iris[order(random),]
head(iris_random)
normal = function(x)(
return((x - min(x)) / (max(x) - min(x)))
)
normal(1:5)
iris_new = as.data.frame(lapply(iris_random[,-5], normal))
summary(iris_new)
train = iris_new[1:130,]
test = iris_new[131:150,]
train_sp = iris_random[1:130,5]
(iris)
(iris)
data(iris)
str(iris)
summary(iris)
set.seed(1234)
head(iris)
random = runif(150)
iris_random = iris[order(random),]
head(iris_random)
normal = function(x)(
return((x - min(x)) / (max(x) - min(x)))
)
normal(1:5)
iris_new = as.data.frame(lapply(iris_random[,-5], normal))
summary(iris_new)
train = iris_new[1:130,]
test = iris_new[131:150,]
# Sin características
train_sc = iris_random[1:130,5]
install.packages("ggplot2")
library(ggplot2)
data(iris)
str(iris)
summary(iris)
set.seed(1234)
head(iris)
random = runif(150)
iris_random = iris[order(random),]
head(iris_random)
normal = function(x)(
return((x - min(x)) / (max(x) - min(x)))
)
normal(1:5)
iris_new = as.data.frame(lapply(iris_random[,-5], normal))
summary(iris_new)
train = iris_new[1:130,]
test = iris_new[131:150,]
# Sin características
train_sc = iris_random[1:130,5]
test_sc = iris_random[131:150,5]
library(class)
model = knn(train = train, test = test, cl = train_sc, k=13)
table(factor(model))
table(test_sp,model)
library(ggplot2)
ggplot(aes(iris_random$Sepal.Length, iris_random$Petal.Width), data = iris_random) + geom_point(aes(color= factor(iris_random$Species)))
## Distancia euclideana y manhattan
library(caret)
library(stats)
setwd("C:\\Users\\ISND89\\Documents\\Ejercicios R")   #Directorio actual de trabajo (directorio en donde est?n el script y los datos).
df.train<-read.table("glass-train.csv",sep =",", stringsAsFactors=FALSE, header = TRUE)
df.test<-read.table("glass-test.csv",sep =",", stringsAsFactors=FALSE, header = TRUE)
## Distancia euclideana y manhattan
library(caret)
library(stats)
setwd("C:\\Users\\ISND89\\Documents\\Ejercicios R")   #Directorio actual de trabajo (directorio en donde est?n el script y los datos).
df.train<-read.table("glass-train.csv",sep =",", stringsAsFactors=FALSE, header = TRUE)
df.test<-read.table("glass-test.csv",sep =",", stringsAsFactors=FALSE, header = TRUE)
df.train<-rbind(df.train, df.test[1,])
df.test<-df.test[-1,]
#################### x vectores ##################
calcula.dist <- function(x, tipo) {
if (tipo == 1) #euclidean
{
res<-sqrt(sum((x[1,] - x[2,])^2))
}
if (tipo == 2) #manhattan
{
res<-sum(abs(x[1,] - x[2,]))
}
res
}
dist.euclidean<-matrix(ncol=5, nrow= nrow(df.train))
for (j in 1:nrow(df.test))
for (j in 1:nrow(df.test)) {
for (i in 1:nrow(df.train))
{
m1<-rbind(df.train[i,1:9], df.test[j,1:9])
dist.euclidean[i,j]<-dist(m1, method = "manhattan")
}
}
## Distancia euclideana y manhattan
library(caret)
library(stats)
setwd("C:\\Users\\ISND89\\Documents\\Ejercicios R")   #Directorio actual de trabajo (directorio en donde est?n el script y los datos).
df.train<-read.table("glass-train.csv",sep =",", stringsAsFactors=FALSE, header = TRUE)
df.test<-read.table("glass-test.csv",sep =",", stringsAsFactors=FALSE, header = TRUE)
df.train<-rbind(df.train, df.test[1,])
df.test<-df.test[-1,]
#################### x vectores ##################
calcula.dist <- function(x, tipo) {
if (tipo == 1) #euclidean
{
res<-sqrt(sum((x[1,] - x[2,])^2))
}
if (tipo == 2) #manhattan
{
res<-sum(abs(x[1,] - x[2,]))
}
res
}
dist.euclidean<-matrix(ncol=5, nrow= nrow(df.train))
for (j in 1:nrow(df.test)) {
for (i in 1:nrow(df.train))
{
m1<-rbind(df.train[i,1:9], df.test[j,1:9])
dist.euclidean[i,j]<-calcula.dist(m1, 2)
}
}
k<-5
knn.results<-matrix(nrow = nrow(df.test), ncol = k)
knn.distances<-matrix(nrow = nrow(df.test), ncol = k)
for(i in 1:nrow(df.test))
{
knn.results[i,]<-order(dist.euclidean[,i])[1:k]
knn.distances[i,]<-dist.euclidean[knn.results[i,],i]
}
View(knn.distances)
View(knn.results)
#Obtener la clases de los k vecinos
knn.clases<-matrix(nrow = nrow(df.test), ncol = k)
clase.asignada<-NULL
for(i in 1:nrow(df.test))
{
knn.clases[i,]<-df.train[knn.results[i,],'Type']
clase.asignada<-rbind(clase.asignada, max(knn.clases[i,]))
}
cbind(clase.asignada,(df.test[,'Type']))
table(clase.asignada, (df.test[,'Type']))
View(dist.euclidean)
# Boston dataset from MASS
forestF <- read.csv("C:\Users\ISND89\Documents\ISND 8°\Machine Learning", header = TRUE)
# Boston dataset from MASS
forestF <- read.csv("C:/Users/ISND89/Documents/ISND 8°/Machine Learning/forestfires.csv", header = TRUE)
View(forestF)
# Import Required packages
set.seed(500)
library(neuralnet)
library(MASS)
# Boston dataset from MASS
data <- Boston
View(data)
# Boston dataset from MASS
forestF <- read.csv("C:/Users/ISND89/Documents/ISND 8°/Machine Learning/forestfires.csv", header = TRUE)
View(forestF)
# Eliminar las columnas "month" y "day"
forestF <- forestF[, c(1:2, 4:13)]
View(forestF)
# Eliminar las columnas "month" y "day"
forestF <- forestF[, c(1:2, 5:13)]
# Eliminar las columnas "month" y "day"
forestF <- forestF[, c(1:2, 5:13)]
# Boston dataset from MASS
forestF <- read.csv("C:/Users/ISND89/Documents/ISND 8°/Machine Learning/forestfires.csv", header = TRUE)
# Eliminar las columnas "month" y "day"
forestF <- forestF[, c(1:2, 5:13)]
View(forestF)
# Import Required packages
# Import Required packages
set.seed(500)
library(neuralnet)
library(MASS)
# Boston dataset from MASS
forestF <- read.csv("C:/Users/ISND89/Documents/ISND 8°/Machine Learning/forestfires.csv", header = TRUE)
# Eliminar las columnas "month" y "day"
forestF <- forestF[, c(1:2, 5:13)]
# Normalize the data
maxs <- apply(forestF, 2, max)
mins <- apply(forestF, 2, min)
scaled <- as.data.frame(scale(forestF, center = mins,
scale = maxs - mins))
# Split the data into training and testing set
index <- sample(1:nrow(forestF), round(0.75 * nrow(forestF)))
train_ <- scaled[index,]
test_ <- scaled[-index,]
# Build Neural Network
nn <- neuralnet(area ~ X + Y + FFMC + DMC + DC
+ ISI + temp + RH + wind + rain +
area,
data = train_, hidden = c(5, 3),
linear.output = TRUE)
# Predict on test data
pr.nn <- compute(nn, test_[,1:13])
# Predict on test data
pr.nn <- compute(nn, test_[,1:11])
# Compute mean squared error
pr.nn_ <- pr.nn$net.result * (max(forestF$area) - min(forestF$area)) + min(forestF$area)
test.r <- (test_$area) * (max(forestF$area) - min(forestF$area)) + min(forestF$area)
MSE.nn <- sum((test.r - pr.nn_)^2) / nrow(test_)
# Plot the neural network
plot(nn)
# Plot regression line
plot(test_$area, pr.nn_, col = "red",
main = 'Real vs Predicted')
abline(0, 1, lwd = 2)
# Plot the neural network
plot(nn)
# Import Required packages
set.seed(500)
library(neuralnet)
library(MASS)
# Boston dataset from MASS
forestF <- read.csv("C:/Users/ISND89/Documents/ISND 8°/Machine Learning/forestfires.csv", header = TRUE)
# Eliminar las columnas "month" y "day"
forestF <- forestF[, c(1:2, 5:13)]
# Normalize the data
maxs <- apply(forestF, 2, max)
mins <- apply(forestF, 2, min)
scaled <- as.data.frame(scale(forestF, center = mins,
scale = maxs - mins))
# Split the data into training and testing set
index <- sample(1:nrow(forestF), round(0.75 * nrow(forestF)))
train_ <- scaled[index,]
test_ <- scaled[-index,]
# Build Neural Network
nn <- neuralnet(area ~ X + Y + FFMC + DMC + DC
+ ISI + temp + RH + wind + rain +
area,
data = train_, hidden = c(5, 3),
linear.output = TRUE)
# Predict on test data
pr.nn <- compute(nn, test_[,1:11])
# Compute mean squared error
pr.nn_ <- pr.nn$net.result * (max(forestF$area) - min(forestF$area)) + min(forestF$area)
test.r <- (test_$area) * (max(forestF$area) - min(forestF$area)) + min(forestF$area)
MSE.nn <- sum((test.r - pr.nn_)^2) / nrow(test_)
# Plot the neural network
plot(nn)
# Plot regression line
plot(test_$area, pr.nn_, col = "red",
main = 'Real vs Predicted')
abline(0, 1, lwd = 2)
View(train_)
View(test_)
